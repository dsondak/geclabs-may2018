{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2: Multiple Linear Regression\n",
    "\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import statsmodels.api as sm\n",
    "import scipy as sp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression from scratch\n",
    "\n",
    "You are provided a data set containing attributes related to automobiles as well as their corresponding prices. The task is to build a linear regression model from scratch that can estimate the miles-per-gallon(mpg) of an automobile (response variable) using its attributes (predictor variables).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Models as Matrices (Arrays)\n",
    "\n",
    "What are the type of objects that regression models we have seen so far work with?\n",
    "\n",
    "![](images/sklearnform.png)\n",
    "\n",
    "Note that $X$ is a two dimensional array with shape (3, 2) and $Y$ is a one dimensional array with shape (3, ) (or (3, 1) as a trivial two dimensional array). \n",
    "\n",
    "We need to turn models (functional forms) into arrays and matrices. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\underbrace{\\hat{Y} = f(X_1, X_2) = 2X_1 - X_2}_{Model} \\longrightarrow X\n",
    "\\end{align}$$\n",
    "\n",
    "Note that the model $M$ is a one dimensional array with shape (2, ) (or (2, 1) as a trivial two dimensional array). Also note that this is not the best-fit regression model, just an arbitrary model we are trying out.\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "\\text{Coef. for $X_1$ }&  2\\\\\n",
    "\\text{Coef. for $X_2$ }& -1\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$M= \\left(\\begin{array}{c}\n",
    "2\\\\\n",
    "-1\\\\\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "We need to phrase the prediction task - applying $f(X_1, X_2) = 2X_1 - X_2$ to the data - in terms of matrix operations.\n",
    "\n",
    "![](images/matmult.png)\n",
    "\n",
    "Finally notice that we have not included any intercept here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Formula\n",
    "\n",
    "We first examine a toy problem, focusing our efforts on fitting a linear model to a small dataset with three observations.  Each observation consists of one predictor $x_i$ and one response $y_i$ for $i = 1, 2, 3$,\n",
    "\n",
    "\\begin{equation*}\n",
    "(x , y) = \\{(x_1, y_1), (x_2, y_2), (x_3, y_3)\\}.\n",
    "\\end{equation*}\n",
    "\n",
    "To be very concrete, let's set the values of the predictors and responses.\n",
    "\n",
    "\\begin{equation*}\n",
    "(x , y) = \\{(1, 2), (2, 2), (3, 4)\\}\n",
    "\\end{equation*}\n",
    "\n",
    "There is no line of the form $\\beta_0 + \\beta_1 x = y$ that passes through all three observations, since the data is not collinear.  Thus our aim is to find the line that best fits these observations in the *least-squares sense*, as discussed in lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suspending reality, suppose there is a line $\\beta_0 + \\beta_1 x = y$ that passes through all three observations.  Then we'd solve\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_0 + \\beta_1 &=& 2 \\nonumber \\\\\n",
    "\\beta_0 + 2 \\beta_1 &=& 2 \\nonumber \\\\\n",
    "\\beta_0 + 3 \\beta_1 &=& 4, \\nonumber \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "for  $\\beta_0$ and  $\\beta_1$, the intercept and slope of the desired line.  Let's write these equations in matrix form.  The left hand sides of the above equations can be written as\n",
    "\n",
    "![?](images/LHS.png)\n",
    "\n",
    "while the right hand side is simply the vector\n",
    "\n",
    "\\begin{equation*}Y = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "4 \n",
    "\\end{bmatrix}. \\end{equation*}\n",
    "\n",
    "Thus we have the matrix equation $X \\beta = Y$ where\n",
    "\n",
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "1 & 1\\\\\n",
    "1 & 2\\\\\n",
    "1 & 3\n",
    "\\end{bmatrix}, \\quad\n",
    "\\beta = \\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \n",
    "\\end{pmatrix}, \\quad \\mathrm{and} \n",
    "\\quad Y = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "4 \n",
    "\\end{bmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "To find the best possible solution to this linear system that has no solution, we need to solve the *normal equations*, or\n",
    "\n",
    "\\begin{equation}\n",
    "X^T X \\beta = X^T Y.\n",
    "\\end{equation}\n",
    "\n",
    "If $X^T X$ is invertible then the solution is\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (X^T X)^{-1} X^T Y.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if there were two predictors?\n",
    "\n",
    "The $X$ matrix and vector $\\beta$ change.  In this case adding a second predictor variable would result in adding a third column to the $X$ matrix, so that the matrix is $3 \\times 3$.  A third variable would be added to the $\\beta$ vector.  Note that we need to be consistent in appending rows and columns to the matrix $X$ and the vector $\\beta$.  For example, if the new predictor column is \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "v_3\n",
    "\\end{bmatrix}, \n",
    "\\end{equation}\n",
    "\n",
    "and we include it in the $X$ matrix as\n",
    "\n",
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "1 & 1 & v_1\\\\\n",
    "1 & 2 & v_2\\\\\n",
    "1 & 3 & v_3\n",
    "\\end{bmatrix},\n",
    "\\end{equation}\n",
    "then the corresponding $\\beta$ vector is\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = \\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\beta_2\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Thus the linear system in matrix form is still $X \\beta = Y$, \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & v_1\\\\\n",
    "1 & 2 & v_2\\\\\n",
    "1 & 3 & v_3\n",
    "\\end{bmatrix} \\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\beta_2\n",
    "\\end{pmatrix} = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "4 \n",
    "\\end{bmatrix}, \n",
    "\\end{equation}\n",
    "\n",
    "which can be expanded as (via matrix multiplication as discussed during lab)\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_0 + \\beta_1 + v_1 \\beta_2&=& 2 \\nonumber \\\\\n",
    "\\beta_0 + 2 \\beta_1 + v_2 \\beta_2 &=& 2 \\nonumber \\\\\n",
    "\\beta_0 + 3 \\beta_1 + v_3 \\beta_2&=& 4. \\nonumber \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Everything else remains the same.  \n",
    "\n",
    "Does the addition of a third column changes the invertibility of $X^T X$.  The answer is that it depends on the particular values of $v_1, v_2$, and $v_3$.  For example, if $v_1 = v_2 = v_3 = 1$, then  $X^T X$ is not invertible.  If $v_1 = v_2 = 1$ and $v_3 = 4$, then $X^T X$ is invertible.  You check this directly by writing some code yourself (find the determinant!).  In the code we've used the determinant of $X^T X$ to check for invertibility.  That is, $X^T X$ is invertible if and only if $\\det(X^T X) \\ne 0$.  \n",
    "\n",
    "*(You can be fancier about this if you like by using the properties of determinants, in the case when $X$ is a square matrix. $\\det X^T X = \\det X^T \\det X = \\det X \\det X = (\\det X)^2$, and since $\\det X = 0$ in the first example, $\\det X^T X = 0$ and hence is not invertible.  For the first example with $v_1 = v_2 = v_3 = 1$ we can see by inspection that $X$ is not invertible since the third column of $X$ is a constant multiple of the first column, ie, the  columns of $X$ do not form a linearly independent set.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression Fit\n",
    "\n",
    "Ok so lets write this in code\n",
    "\n",
    "``multiple_linear_regression_fit``:\n",
    "\n",
    "- takes as input: the training set, ``x_train``, ``y_train``\n",
    "- fits a multiple linear regression model\n",
    "- returns the model parameters (coefficients on the predictors, as an array, and the intercept, as a float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" multiple_linear_regression_fit\n",
    "\n",
    "A function for fitting a multiple linear regression\n",
    "with n responses and d predictors\n",
    "\n",
    "Fitted model: f(x) = x.w + c\n",
    "\n",
    "Input: \n",
    "     x_train (n x d array of predictors in training data)\n",
    "     y_train (n x 1 array of response variable vals in training data)\n",
    "Return: \n",
    "     w (d x 1 array of coefficients) \n",
    "     c (float representing intercept)\n",
    "\"\"\"\n",
    "\n",
    "def multiple_linear_regression_fit(x_train, y_train):\n",
    "    # Append a column of one's to x\n",
    "    n = x_train.shape[0]\n",
    "    ones_col = np.ones((n, 1))\n",
    "    x_train = np.concatenate((x_train, ones_col), axis=1)\n",
    "    \n",
    "    # Compute transpose of x\n",
    "    x_transpose = np.transpose(x_train)\n",
    "    \n",
    "    # Compute coefficients: w = inv(x^T * x) x^T * y\n",
    "    # Compute intermediate term: inv(x^T * x)\n",
    "    # Note: We have to take pseudo-inverse (pinv), just in case x^T * x is not invertible \n",
    "    x_t_x_inv = np.linalg.pinv(np.dot(x_transpose, x_train))\n",
    "    \n",
    "    # Compute w: inter_term * x^T * y \n",
    "    w = np.dot(np.dot(x_t_x_inv, x_transpose), y_train)\n",
    "    \n",
    "    # Obtain intercept: 'c' (last index)\n",
    "    c = w[-1]\n",
    "    \n",
    "    return w[:-1], c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score\n",
    "\n",
    "``multiple_linear_regression_score``:\n",
    "\n",
    "- takes model parameters (coefficients and intercept) and the test set, ``x_test`` ``y_test``, as inputs\n",
    "- returns the $R^2$ score for the model on the test set, along with the predicted y-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"multiple_linear_regression_score\n",
    "\n",
    "A function for evaluating R^2 score and MSE \n",
    "of the linear regression model on a data set\n",
    "with n observations and d predictors\n",
    "\n",
    "Input: \n",
    "     w (d x 1 array of coefficients)\n",
    "     c (float representing intercept)\n",
    "     x_test (n x d array of predictors in testing data)\n",
    "     y_test (n x 1 array of response variable vals in testing data)\n",
    "     \n",
    "Return: \n",
    "     r_squared (float) \n",
    "     y_pred (n x 1 array of predicted y-vals)\n",
    "\"\"\"\n",
    "\n",
    "def multiple_linear_regression_score(w, c, x_test, y_test):\n",
    "    # Compute predicted labels\n",
    "    y_pred = np.dot(x_test, w) + c\n",
    "    \n",
    "    # Evaluate sqaured error, against target labels\n",
    "    # sq_error = \\sum_i (y[i] - y_pred[i])^2\n",
    "    sq_error = np.sum(np.square(y_test - y_pred))\n",
    "    \n",
    "    # Evaluate squared error for a predicting the mean value, against target labels\n",
    "    # variance = \\sum_i (y[i] - y_mean)^2\n",
    "    y_mean = np.mean(y_test)\n",
    "    y_variance = np.sum(np.square(y_test - y_mean))\n",
    "    \n",
    "    # Evaluate R^2 score value\n",
    "    r_squared = 1 - sq_error / y_variance\n",
    "\n",
    "    return r_squared, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your functions\n",
    "\n",
    "We've provided some code below to test our functions. We'll assume  \"perfect\" data generated from the line:\n",
    "\n",
    "$$f(x) = 4x + 5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : 4*x + 5\n",
    "testx_flat = np.arange(10)\n",
    "testy = f(testx_flat)\n",
    "testx = testx_flat.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testx, testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your two functions. Is the output what you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_linear_regression_fit(testx, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_linear_regression_score([4], 5, testx, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the mtcars data\n",
    "\n",
    "Use your functions to predict automobile mpg and evaluate your predictions. We'll use `wt` and `hp` as predictors for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcars=pd.read_csv(\"data/mtcars_lab2.csv\")\n",
    "dfcars=dfcars.rename(columns={\"Unnamed: 0\":\"name\"})\n",
    "dfcars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the shape and dtypes of our data to make sure they make sense, and to get the measure of the `small` data we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcars.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a training and testing dataframe. . Notice how we used the dataframe itself..this is because pandas dataframes implement python's  *array* interface, and thus anything done to an array can be done to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "traindf, testdf = train_test_split(dfcars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the shapes of our training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.shape, testdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Two-Three  Go!\n",
    "\n",
    "We assign training and test $x$ and $y$ based on the respective dataframes. Notice how we use pandas dataframes and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split predictors from response\n",
    "# Training\n",
    "y_train = traindf.mpg\n",
    "x_train = traindf[['wt', 'hp']]\n",
    "\n",
    "# Testing\n",
    "y_test = testdf.mpg\n",
    "x_test = testdf[['wt', 'hp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> YOUR TURN NOW\n",
    ">Fit the training data and calculate the `r_squared` on the test set. Store the list of coefficients in `w` and the intercept in `c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here.\n",
    "# Fit multiple linear regression model\n",
    "\n",
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the $R^2$ out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R^2 score on test set:', r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run this on the training set as well.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_squared_train, _ = multiple_linear_regression_score(w, c, x_train, y_train)\n",
    "\n",
    "print('R^2 score on train set:', r_squared_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try with sklearn\n",
    "\n",
    "Lets do the same process with sklearn. We provide here the code for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import linear model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#create linear model\n",
    "regression = LinearRegression()\n",
    "\n",
    "#fit linear model\n",
    "regression.fit(x_train, y_train)\n",
    "\n",
    "#predict y-values\n",
    "predicted_y = regression.predict(x_test)\n",
    "\n",
    "#score predictions (sklearn gives you R^2 as well)\n",
    "r = regression.score(x_test, y_test)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now try with statsmodels\n",
    "\n",
    "Statsmodels procides a more R-like, or statisticsy interface which will illustrate a lot of the concepts we discussed in the second lecture.\n",
    "\n",
    "Its interface is fifferent from sklearn in that you need to add a column of 1's explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column of ones to x matrix\n",
    "xtrain_vals = sm.add_constant(x_train.values)\n",
    "ytrain_vals = y_train.values\n",
    "# Create model for linear regression\n",
    "model = sm.OLS(ytrain_vals, xtrain_vals)\n",
    "# Fit model\n",
    "fitted_model = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From the R-cookbook*:\n",
    "\n",
    ">...theoretically, if a variable’s coefficient is zero then the variable is worthless; it adds nothing to the model. Yet the coefficients shown here are only estimates, and they will never be exactly zero. We therefore ask: Statistically speaking, how likely is it that the true coefficient is zero? That is the purpose of the t statistics and the p-values, which in the summary are labeled (respectively) t value and Pr(>|t|).\n",
    "The p-value is a probability. It gauges the likelihood that the coefficient is not significant, so smaller is better. Big is bad because it indicates a high likelihood of insignificance. In this example, the p-value for the u coefficient is a mere 0.00106, so u is likely significant....Variables with large p-values are candidates for elimination.\n",
    "\n",
    ">$R^2 $is a measure of the model’s quality. Bigger is better. Mathematically, it is the fraction of the variance of y that is explained by the regression model. The remain- ing variance is not explained by the model, so it must be due to other factors (i.e., unknown variables or sampling variability).... That being said, I strongly suggest using the adjusted rather than the basic $R^2$. The adjusted value accounts for the number of variables in your model and so is a more realistic assessment of its effectiveness. \n",
    "\n",
    ">The F statistic tells you whether the model is significant or insignificant. The model is significant if any of the coefficients are nonzero (i.e., if βi ≠ 0 for some i). It is insignificant if all coefficients are zero (β1 = β2 = ... = βn = 0).\n",
    "\n",
    ">Conventionally, a p-value of less than 0.05 indicates that the model is likely significant (one or more βi are nonzero) whereas values exceeding 0.05 indicate that the model is likely not significant. \n",
    "\n",
    ">Most people look at the $R^2$ statistic first. The statistician wisely starts with the F statistic, for if the model is not significant then nothing else matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> YOUR TURN NOW\n",
    "\n",
    ">Notice the t-values and the p values (P > |t|) corresponding to those t-values. Are the coefficients of x1 and x2 statistically significant?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> YOUR TURN NOW\n",
    "\n",
    ">Notice that statsmodels also gives us the confidence intervals correspnding to 5%, with 2.5% on each side. Which of the x1 and x2 coefficients seems more significant?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally notice the F-statistic. What does it tell us about the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence interval on regression parameters\n",
    "\n",
    "Using your linear regression implementation from above model, you can compute confidence intervals for the model parameters you obtain:\n",
    "\n",
    "- Create 200 random subsamples of the data set (with replacement) of size 10, size 18, and then of size 24 (your training data set size), and use your function to fit a multiple linear regression model to each subsample. \n",
    "\n",
    "- For each coefficient on the predictor variables: we will plot a histogram of the values obtained across the subsamples, and calculate the confidence interval for the coefficients at a confidence level of 95%. \n",
    "\n",
    "- Think about how these compare to the confidence intervals you get from `statsmodels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do by Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 200 #number of subsamples we will use\n",
    "d=2 #since we are fitting 2 parameters in addition to the intercept\n",
    "n=x_train.shape[0] #the sample size in each subsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a function `do_bootstrap` which:\n",
    "\n",
    "- takes as an argument the `subsample_size`, and creates `num_samples` samples\n",
    "\n",
    "> YOUR TURN HERE\n",
    "\n",
    ">Complete the function to \n",
    "- carry out the regression on each one of the samples\n",
    "- returns a `(num_samples x  d)` size array of computed coefficients from the `num_samples` regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_bootstrap(subsample_size):\n",
    "    coefs_multiple = np.zeros((num_samples, d))\n",
    "    for i in range(num_samples):\n",
    "        perm = np.random.choice(range(n), size=subsample_size, replace=True) \n",
    "        \n",
    "        x_subsample = x_train.values[perm, :] \n",
    "        y_subsample = y_train.values[perm]\n",
    "        # your code here\n",
    "    return coefs_multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a function to plot the confidence intervals for the coefficients from the output of the bootstrap function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ci(coefs_multiple):\n",
    "    with sns.plotting_context('poster'):\n",
    "        fig, axes = plt.subplots(1, d)#, figsize=(20, 8))\n",
    "\n",
    "        # Repeat for each coefficient\n",
    "        for j in range(d):\n",
    "            # Compute mean for the j-th coefficent from subsamples\n",
    "            coef_j_mean = np.mean(coefs_multiple[:, j])\n",
    "\n",
    "            # Compute confidence interval at 95% confidence level (use formula!)\n",
    "            conf_int_left = np.percentile(coefs_multiple[:, j], 2.5)\n",
    "            conf_int_right = np.percentile(coefs_multiple[:, j], 97.5)\n",
    "\n",
    "            # Plot histogram of coefficient values\n",
    "            axes[j].hist(coefs_multiple[:, j], alpha=0.5)\n",
    "\n",
    "            # Plot vertical lines at mean and left, right extremes of confidence interval\n",
    "            axes[j].axvline(x = coef_j_mean, linewidth=3)\n",
    "            axes[j].axvline(x = conf_int_left, linewidth=1, c='r')\n",
    "            axes[j].axvline(x = conf_int_right, linewidth=1, c='r')\n",
    "\n",
    "            # Set plot labels\n",
    "            axes[j].set_title('[' + str(round(conf_int_left, 4)) \n",
    "                              + ', ' \n",
    "                              + str(round(conf_int_right, 4)) + ']')\n",
    "            axes[j].set_xlabel('Predictor ' + str(j + 1))\n",
    "            axes[j].set_ylabel('Frequency')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> YOUR TURN NOW\n",
    "\n",
    ">Do the bootstrap for size 10, size 18 and size 24. What trend do you see as the sample size increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for size 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here for size 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
